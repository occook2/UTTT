# AlphaZero Training Configuration

# Training parameters
n_epochs: 2
games_per_epoch: 10
batch_size: 128
learning_rate: 0.001
weight_decay: 0.0001

# Adaptive learning rate parameters
use_lr_scheduler: false
lr_scheduler_patience: 3     # Epochs to wait before reducing LR
lr_scheduler_factor: 0.5     # Factor to multiply LR by
lr_scheduler_threshold: 0.01 # Minimum decrease to be significant

# Self-play parameters
mcts_simulations: 50
temperature_threshold: 9
temperature: 1.0
c_puct: 1.0
use_multiprocessing: true
num_processes: 5

# Adaptive temperature based on policy confidence
use_adaptive_temperature: false
confidence_threshold: 0.6    # Switch to deterministic if max policy > 60%

# Dirichlet noise for exploration during training
add_dirichlet_noise: false
dirichlet_alpha: 0.3        # Good for TTT (branching factor ~9)
dirichlet_epsilon: 0.25     # Mix 75% prior + 25% noise
dirichlet_noise_moves: 10   # Only apply noise to first N moves of game

# Model saving
save_every: 1
checkpoint_dir: "checkpoints"

# Device
device: "cuda"

# Training data management
max_training_samples: 100000

# Symmetry augmentation
use_symmetry_augmentation: false

# UI data saving (separate from training)
save_ui_data: true

# Neural Network Architecture
network:
  in_planes: 5          # Input channels (TTT state representation)
  channels: 32          # Channels in convolutional layers
  blocks: 2             # Number of residual blocks  
  board_n: 3            # Board size (TTT is 3x3)
  policy_reduce: 16     # Policy head
  value_hidden: 64      # Value head