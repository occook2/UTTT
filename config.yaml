# AlphaZero Training Configuration

# Training parameters
n_epochs: 2
games_per_epoch: 5
batch_size: 64
learning_rate: 0.001
weight_decay: 0.0001

# Adaptive learning rate parameters
use_lr_scheduler: true
lr_scheduler_patience: 3     # Epochs to wait before reducing LR
lr_scheduler_factor: 0.5     # Factor to multiply LR by
lr_scheduler_threshold: 0.01 # Minimum decrease to be significant

# Self-play parameters
mcts_simulations: 10
temperature_threshold: 81
temperature: 1.0
c_puct: 1.0
use_multiprocessing: true
num_processes: 5

# Adaptive temperature based on policy confidence
use_adaptive_temperature: true
confidence_threshold: 0.6    # Switch to deterministic if max policy > 60%

# Dirichlet noise for exploration during training
add_dirichlet_noise: true
dirichlet_alpha: 0.3        # Good for UTTT (branching factor ~9-81)
dirichlet_epsilon: 0.25     # Mix 75% prior + 25% noise
dirichlet_noise_moves: 10   # Only apply noise to first N moves of game

# Model saving
save_every: 2
checkpoint_dir: "checkpoints"

# Device
device: "cuda"

# Training data management
max_training_samples: 100000

# Symmetry augmentation
use_symmetry_augmentation: true

# UI data saving (separate from training)
save_ui_data: true

# Neural Network Architecture
network:
  in_planes: 7          # Input channels (UTTT state representation)
  channels: 32          # Channels in convolutional layers
  blocks: 3             # Number of residual blocks  
  board_n: 9            # Board size (UTTT is 9x9)
  policy_reduce: 16     # Policy head
  value_hidden: 64      # Value head